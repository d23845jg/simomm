{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjinja2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Environment, FileSystemLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     31\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1507\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1479\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1630\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:126\u001b[0m, in \u001b[0;36m_path_join\u001b[0;34m(*path_parts)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "from training.create_dataset import *\n",
    "from training.create_network import *\n",
    "from training.utils import create_task_flags, TaskMetric, compute_loss, get_weight_str, eval\n",
    "from utils import torch_save, get_data_loaders, initialize_wandb\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for training\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "template = env.get_template('config/mtl.yaml.j2')\n",
    "rendered_yaml = template.render()\n",
    "mtl_config = yaml.safe_load(rendered_yaml)\n",
    "\n",
    "# Create logging folder to store training weights and losses\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "model_classes = {\n",
    "  \"split\": MTLDeepLabv3,\n",
    "  \"mtan\": MTANDeepLabv3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_wandb(\n",
    "  project=mtl_config[\"wandb\"][\"project\"], \n",
    "  group=f\"{mtl_config['training_params']['network']}\", \n",
    "  job_type=\"mtl\", \n",
    "  mode=mtl_config[\"wandb\"][\"mode\"], \n",
    "  config={\n",
    "    \"task\": mtl_config['training_params']['task'],\n",
    "    \"network\": mtl_config['training_params']['network'],\n",
    "    \"dataset\": mtl_config['training_params']['dataset'],\n",
    "    \"weight\": mtl_config['training_params']['weight'],\n",
    "    \"epochs\": mtl_config['training_params']['total_epochs'],\n",
    "    \"lr\": mtl_config['training_params']['lr'],\n",
    "    \"batch_size\": mtl_config['training_params']['batch_size'],\n",
    "    \"seed\": mtl_config['training_params']['seed'],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(mtl_config[\"training_params\"][\"seed\"])\n",
    "np.random.seed(mtl_config[\"training_params\"][\"seed\"])\n",
    "random.seed(mtl_config[\"training_params\"][\"seed\"])\n",
    "\n",
    "# device = torch.device(f\"cuda:{mtl_config[\"training_params\"]['gpu']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = create_task_flags('all', mtl_config[\"training_params\"][\"dataset\"], with_noise=mtl_config[\"training_params\"][\"with_noise\"])\n",
    "pri_tasks = create_task_flags(mtl_config[\"training_params\"][\"task\"], mtl_config[\"training_params\"][\"dataset\"], with_noise=False)\n",
    "\n",
    "train_tasks_str = ' + '.join(task.title() for task in train_tasks.keys())\n",
    "pri_tasks_str = ' + '.join(task.title() for task in pri_tasks.keys())\n",
    "print(f\"Dataset: {mtl_config['training_params']['dataset'].title()} | Training Task: {train_tasks_str} | Primary Task: {pri_tasks_str} in Multi-task / Auxiliary Learning Mode with {mtl_config['training_params']['network'].upper()}\")\n",
    "print(f\"Applying Multi-task Methods | Weighting-based: {mtl_config['training_params']['weight'].title()} + Gradient-based: {mtl_config['training_params']['grad_method'].upper()}\")\n",
    "\n",
    "# Initialize model\n",
    "model = model_classes[mtl_config[\"training_params\"][\"network\"]](train_tasks).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {mtl_config['training_params']['network'].title()} | Number of Trainable Parameters: {num_params/1e6:.2f}M\")\n",
    "\n",
    "# Choose task weighting\n",
    "params = model.parameters()\n",
    "if mtl_config[\"training_params\"][\"weight\"] == \"uncert\":\n",
    "    logsigma = torch.tensor([-0.7] * len(train_tasks), requires_grad=True, device=device)\n",
    "    params = list(params) + [logsigma]\n",
    "    logsigma_ls = np.zeros((mtl_config[\"training_params\"][\"total_epochs\"], len(train_tasks)), dtype=np.float32)\n",
    "\n",
    "elif mtl_config[\"training_params\"][\"weight\"] in [\"dwa\", \"equal\"]:\n",
    "    T = 2.0  # Temperature used in DWA\n",
    "    lambda_weight = np.ones((mtl_config[\"training_params\"][\"total_epochs\"], len(train_tasks)))\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.SGD(params, lr=mtl_config[\"training_params\"][\"lr\"], weight_decay=1e-4, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, mtl_config[\"training_params\"][\"total_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(mtl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply gradient methods\n",
    "if mtl_config[\"training_params\"][\"grad_method\"] != 'none':\n",
    "    rng = np.random.default_rng()\n",
    "    grad_dims = []\n",
    "    for mm in model.shared_modules():\n",
    "        for param in mm.parameters():\n",
    "            grad_dims.append(param.data.numel())\n",
    "    grads = torch.Tensor(sum(grad_dims), len(train_tasks)).to(device)\n",
    "\n",
    "\n",
    "# Train and evaluate multi-task network\n",
    "train_batch = len(train_loader)\n",
    "test_batch = len(test_loader)\n",
    "train_metric = TaskMetric(train_tasks, pri_tasks, mtl_config[\"training_params\"][\"batch_size\"], mtl_config[\"training_params\"][\"total_epochs\"], mtl_config[\"training_params\"][\"dataset\"])\n",
    "test_metric = TaskMetric(train_tasks, pri_tasks, mtl_config[\"training_params\"][\"batch_size\"], mtl_config[\"training_params\"][\"total_epochs\"], mtl_config[\"training_params\"][\"dataset\"], include_mtl=True)\n",
    "\n",
    "# Training loop\n",
    "for index in range(mtl_config[\"training_params\"][\"total_epochs\"]):\n",
    "    \n",
    "    # iteration for all batches\n",
    "    model.train()\n",
    "    train_dataset = iter(train_loader)\n",
    "    for k in range(train_batch):\n",
    "        train_data, train_target = next(train_dataset)\n",
    "        train_data = train_data.to(device)\n",
    "        train_target = {task_id: train_target[task_id].to(device) for task_id in train_tasks.keys()}\n",
    "\n",
    "        # update multi-task network parameters with task weights\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(train_data)\n",
    "        train_loss = [compute_loss(train_pred[i], train_target[task_id], task_id) for i, task_id in enumerate(train_tasks)]\n",
    "\n",
    "        train_loss_tmp = [0] * len(train_tasks)\n",
    "\n",
    "        if mtl_config[\"training_params\"][\"weight\"] in [\"equal\", \"dwa\"]:\n",
    "            train_loss_tmp = [w * train_loss[i] for i, w in enumerate(lambda_weight[index])]\n",
    "\n",
    "        if mtl_config[\"training_params\"][\"weight\"] == \"uncert\":\n",
    "            train_loss_tmp = [1 / (2 * torch.exp(w)) * train_loss[i] + w / 2 for i, w in enumerate(logsigma)]\n",
    "\n",
    "        loss = sum(train_loss_tmp)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_metric.update_metric(train_pred, train_target, train_loss)\n",
    "\n",
    "    train_str = train_metric.compute_metric()\n",
    "    wandb.log({\n",
    "        **{f\"train/loss/{task_id}\": train_loss[i] for i, task_id in enumerate(train_tasks)},\n",
    "        **{f\"train/metric/{task_id}\": train_metric.get_metric(task_id) for task_id in train_tasks}\n",
    "    }, step=index)\n",
    "    train_metric.reset()\n",
    "\n",
    "    # evaluating\n",
    "    test_str = eval(index, model, test_loader, test_metric)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {index:04d} | TRAIN:{train_str} || TEST:{test_str} | Best: {mtl_config['training_params']['task'].title()} {test_metric.get_best_performance(mtl_config['training_params']['task']):.4f}\")\n",
    "\n",
    "    if mtl_config[\"training_params\"][\"weight\"] in [\"dwa\", \"equal\"]:\n",
    "        dict = {\"train_loss\": train_metric.metric, \"test_loss\": test_metric.metric, \"weight\": lambda_weight}\n",
    "\n",
    "        print(get_weight_str(lambda_weight[index], train_tasks))\n",
    "\n",
    "    if mtl_config[\"training_params\"][\"weight\"] == \"uncert\":\n",
    "        logsigma_ls[index] = logsigma.detach().cpu()\n",
    "        dict = {\"train_loss\": train_metric.metric, \"test_loss\": test_metric.metric, \"weight\": logsigma_ls}\n",
    "\n",
    "        print(get_weight_str(1 / (2 * np.exp(logsigma_ls[index])), train_tasks))\n",
    "\n",
    "    np.save('logging/mtl_dense_{}_{}_{}_{}_{}_{}_.npy'.format(mtl_config[\"training_params\"][\"network\"], mtl_config[\"training_params\"][\"dataset\"], mtl_config[\"training_params\"][\"task\"], mtl_config[\"training_params\"][\"weight\"], mtl_config[\"training_params\"][\"grad_method\"], mtl_config[\"training_params\"][\"seed\"]), dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_save(model, \"models/mtl_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
