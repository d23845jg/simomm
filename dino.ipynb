{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjuan-garciagiraldo\u001b[0m (\u001b[33mjuagarci\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "from training.create_dataset import *\n",
    "from training.create_network import *\n",
    "from training.utils import create_task_flags, TaskMetric, eval\n",
    "from utils import torch_save, get_data_loaders, initialize_wandb\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for training\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "template = env.get_template('config/mtl.yaml.j2')\n",
    "rendered_yaml = template.render()\n",
    "config = yaml.safe_load(rendered_yaml)\n",
    "\n",
    "# Create logging folder to store training weights and losses\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "model_classes = {\n",
    "  \"split\": MTLDeepLabv3,\n",
    "  \"mtan\": MTANDeepLabv3,\n",
    "  # \"dinov2\": MTLDinoVisionTransformer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>logs/wandb/run-20241010_092152-s8yy50bu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/juagarci/simomm/runs/s8yy50bu' target=\"_blank\">vibrant-paper-40</a></strong> to <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">https://wandb.ai/juagarci/simomm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/juagarci/simomm/runs/s8yy50bu' target=\"_blank\">https://wandb.ai/juagarci/simomm/runs/s8yy50bu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'wandb' from '/opt/conda/lib/python3.10/site-packages/wandb/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_wandb(\n",
    "  project=config[\"wandb\"][\"project\"], \n",
    "  group=f\"{config['training_params']['network']}\", \n",
    "  job_type=\"task_specific\", \n",
    "  mode=config[\"wandb\"][\"mode\"], \n",
    "  config={\n",
    "    \"task\": config['training_params']['task'],\n",
    "    \"network\": config['training_params']['network'],\n",
    "    \"dataset\": config['training_params']['dataset'],\n",
    "    \"epochs\": config['training_params']['total_epochs'],\n",
    "    \"lr\": config['training_params']['lr'],\n",
    "    \"batch_size\": config['training_params']['batch_size'],\n",
    "    \"seed\": config['training_params']['seed'],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config[\"training_params\"][\"seed\"])\n",
    "np.random.seed(config[\"training_params\"][\"seed\"])\n",
    "random.seed(config[\"training_params\"][\"seed\"])\n",
    "\n",
    "# device = torch.device(f\"cuda:{config[\"training_params\"]['gpu']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lts4/scratch/students/juagarci/simomm/models/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/mnt/lts4/scratch/students/juagarci/simomm/models/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/mnt/lts4/scratch/students/juagarci/simomm/models/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Task: Nyuv2 - Seg in Single Task Learning Mode with SPLIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lts4/scratch/students/juagarci/simomm/models/dinov2/losses/cross_entropy_loss.py:220: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Split | Number of Trainable Parameters: 86.63M\n"
     ]
    }
   ],
   "source": [
    "from models.dinov2.mtl.multitasker import MTLDinoV2\n",
    "\n",
    "# Initialize model\n",
    "train_tasks = create_task_flags(config[\"training_params\"][\"task\"], config[\"training_params\"][\"dataset\"])\n",
    "print(f\"Training Task: {config['training_params']['dataset'].title()} - {config['training_params']['task'].title()} in Single Task Learning Mode with {config['training_params']['network'].upper()}\")\n",
    "\n",
    "model = MTLDinoV2(\n",
    "  arch_name=\"vit_base\",\n",
    "  head_tasks=train_tasks,\n",
    ")\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {config['training_params']['network'].title()} | Number of Trainable Parameters: {num_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_shared_layers()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"training_params\"][\"lr\"], weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config[\"training_params\"][\"lr\"], steps_per_epoch=len(train_loader), epochs=config[\"training_params\"][\"total_epochs\"],  pct_start=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = len(train_loader)\n",
    "test_batch = len(test_loader)\n",
    "train_metric = TaskMetric(train_tasks, train_tasks, config[\"training_params\"][\"batch_size\"], config[\"training_params\"][\"total_epochs\"], config[\"training_params\"][\"dataset\"])\n",
    "test_metric = TaskMetric(train_tasks, train_tasks, config[\"training_params\"][\"batch_size\"], config[\"training_params\"][\"total_epochs\"], config[\"training_params\"][\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 | TRAIN: Seg 1.3317 0.3201 || TEST: Seg 0.7364 0.5711 | Best: Seg 0.5711\n",
      "Epoch 0001 | TRAIN: Seg 0.5917 0.6254 || TEST: Seg 0.6129 0.6041 | Best: Seg 0.6041\n",
      "Epoch 0002 | TRAIN: Seg 0.5039 0.6507 || TEST: Seg 0.5720 0.6051 | Best: Seg 0.6051\n",
      "Epoch 0003 | TRAIN: Seg 0.4711 0.6662 || TEST: Seg 0.5534 0.6185 | Best: Seg 0.6185\n",
      "Epoch 0004 | TRAIN: Seg 0.4668 0.6641 || TEST: Seg 0.5553 0.6185 | Best: Seg 0.6185\n"
     ]
    }
   ],
   "source": [
    "#  Training loop\n",
    "model.to(device)\n",
    "for epoch in range(config[\"training_params\"][\"total_epochs\"]):\n",
    "    # training\n",
    "    model.train()\n",
    "    train_dataset = iter(train_loader)\n",
    "    for k in range(train_batch):\n",
    "        train_data, train_target = next(train_dataset)\n",
    "        train_data = train_data.to(device)\n",
    "        train_target = {task_id: train_target[task_id].to(device) for task_id in model.head_tasks}\n",
    "        \n",
    "        train_res = model(train_data, None, img_gt=train_target, return_loss=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_res[\"total_loss\"].backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_metric.update_metric(train_res, train_target)\n",
    "    \n",
    "    train_str = train_metric.compute_metric()\n",
    "    \n",
    "    wandb.log({\n",
    "        **{f\"train/loss/{task_id}\": train_res[task_id][\"total_loss\"] for task_id in model.head_tasks},\n",
    "        **{f\"train/metric/{task_id}\": train_metric.get_metric(task_id) for task_id in model.head_tasks}\n",
    "    },) # step=epoch\n",
    "    train_metric.reset()\n",
    "\n",
    "    # evaluating\n",
    "    test_str = eval(epoch, model, test_loader, test_metric)\n",
    "\n",
    "    print(f\"Epoch {epoch:04d} | TRAIN:{train_str} || TEST:{test_str} | Best: {config['training_params']['task'].title()} {test_metric.get_best_performance(config['training_params']['task']):.4f}\")\n",
    "\n",
    "    # task_dict = {\"train_loss\": train_metric.metric, \"test_loss\": test_metric.metric}\n",
    "    # np.save(\"logging/stl_{}_{}_{}_{}.npy\".format(config[\"training_params\"][\"network\"], config[\"training_params\"][\"dataset\"], config[\"training_params\"][\"task\"], config[\"training_params\"][\"seed\"]), task_dict)\n",
    "    torch_save(model, \"checkpoints/dinov2/linear_probing/depth_head_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = len(train_loader)\n",
    "test_batch = len(test_loader)\n",
    "train_metric = TaskMetric(train_tasks, train_tasks, config[\"training_params\"][\"batch_size\"], 5*config[\"training_params\"][\"total_epochs\"], config[\"training_params\"][\"dataset\"])\n",
    "test_metric = TaskMetric(train_tasks, train_tasks, config[\"training_params\"][\"batch_size\"], 5*config[\"training_params\"][\"total_epochs\"], config[\"training_params\"][\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_shared_layers(requires_grad=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"training_params\"][\"lr\"], weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config[\"training_params\"][\"lr\"], steps_per_epoch=len(train_loader), epochs=5*config[\"training_params\"][\"total_epochs\"],  pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 | TRAIN: Seg 1.4587 0.2263 || TEST: Seg 4.1883 0.0324 | Best: Seg 0.0324\n",
      "Epoch 0001 | TRAIN: Seg 1.9054 0.0679 || TEST: Seg 1.7768 0.0664 | Best: Seg 0.0664\n",
      "Epoch 0002 | TRAIN: Seg 1.7668 0.0832 || TEST: Seg 1.8019 0.0513 | Best: Seg 0.0664\n",
      "Epoch 0003 | TRAIN: Seg 1.7591 0.0852 || TEST: Seg 1.8789 0.0808 | Best: Seg 0.0808\n",
      "Epoch 0004 | TRAIN: Seg 1.7298 0.0885 || TEST: Seg 1.9900 0.0658 | Best: Seg 0.0808\n",
      "Epoch 0005 | TRAIN: Seg 1.6877 0.0917 || TEST: Seg 1.8294 0.0719 | Best: Seg 0.0808\n",
      "Epoch 0006 | TRAIN: Seg 1.6428 0.0978 || TEST: Seg 1.7732 0.0854 | Best: Seg 0.0854\n",
      "Epoch 0007 | TRAIN: Seg 1.6471 0.0977 || TEST: Seg 2.0463 0.0515 | Best: Seg 0.0854\n",
      "Epoch 0008 | TRAIN: Seg 1.5888 0.1008 || TEST: Seg 1.7278 0.0782 | Best: Seg 0.0854\n",
      "Epoch 0009 | TRAIN: Seg 1.5584 0.1055 || TEST: Seg 1.9291 0.0693 | Best: Seg 0.0854\n",
      "Epoch 0010 | TRAIN: Seg 1.5267 0.1158 || TEST: Seg 1.7627 0.0712 | Best: Seg 0.0854\n",
      "Epoch 0011 | TRAIN: Seg 1.5166 0.1207 || TEST: Seg 1.9027 0.0598 | Best: Seg 0.0854\n",
      "Epoch 0012 | TRAIN: Seg 1.4689 0.1240 || TEST: Seg 1.6878 0.0692 | Best: Seg 0.0854\n",
      "Epoch 0013 | TRAIN: Seg 1.4944 0.1209 || TEST: Seg 1.6752 0.0947 | Best: Seg 0.0947\n",
      "Epoch 0014 | TRAIN: Seg 1.4485 0.1252 || TEST: Seg 1.6251 0.0923 | Best: Seg 0.0947\n",
      "Epoch 0015 | TRAIN: Seg 1.4436 0.1297 || TEST: Seg 1.6529 0.0863 | Best: Seg 0.0947\n",
      "Epoch 0016 | TRAIN: Seg 1.4195 0.1477 || TEST: Seg 1.6275 0.1029 | Best: Seg 0.1029\n",
      "Epoch 0017 | TRAIN: Seg 1.4097 0.1455 || TEST: Seg 1.6260 0.1133 | Best: Seg 0.1133\n",
      "Epoch 0018 | TRAIN: Seg 1.3503 0.1557 || TEST: Seg 1.6253 0.1073 | Best: Seg 0.1133\n",
      "Epoch 0019 | TRAIN: Seg 1.3245 0.1803 || TEST: Seg 1.6181 0.0946 | Best: Seg 0.1133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m train_str \u001b[38;5;241m=\u001b[39m train_metric\u001b[38;5;241m.\u001b[39mcompute_metric()\n\u001b[1;32m     23\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: train_res[task_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhead_tasks},\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/metric/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: train_metric\u001b[38;5;241m.\u001b[39mget_metric(task_id) \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhead_tasks}\n\u001b[1;32m     26\u001b[0m },) \u001b[38;5;66;03m# step=epoch\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/lts4/scratch/students/juagarci/simomm/training/utils.py:90\u001b[0m, in \u001b[0;36mTaskMetric.update_metric\u001b[0;34m(self, train_res, task_gt)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_id, gt \u001b[38;5;129;01min\u001b[39;00m task_gt\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     89\u001b[0m     pred \u001b[38;5;241m=\u001b[39m train_res[task_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric[task_id][e, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric[task_id][e, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m*\u001b[39m \u001b[43mtrain_res\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart_seg\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m# update confusion matrix (metric will be computed directly in the Confusion Matrix)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf_mtx[task_id]\u001b[38;5;241m.\u001b[39mupdate(pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(), gt\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  Training loop\n",
    "model.to(device)\n",
    "for epoch in range(5*config[\"training_params\"][\"total_epochs\"]):\n",
    "    # training\n",
    "    model.train()\n",
    "    train_dataset = iter(train_loader)\n",
    "    for k in range(train_batch):\n",
    "        train_data, train_target = next(train_dataset)\n",
    "        train_data = train_data.to(device)\n",
    "        train_target = {task_id: train_target[task_id].to(device) for task_id in model.head_tasks}\n",
    "        \n",
    "        train_res = model(train_data, None, img_gt=train_target, return_loss=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_res[\"total_loss\"].backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_metric.update_metric(train_res, train_target)\n",
    "    \n",
    "    train_str = train_metric.compute_metric()\n",
    "    \n",
    "    wandb.log({\n",
    "        **{f\"train/loss/{task_id}\": train_res[task_id][\"total_loss\"] for task_id in model.head_tasks},\n",
    "        **{f\"train/metric/{task_id}\": train_metric.get_metric(task_id) for task_id in model.head_tasks}\n",
    "    },) # step=epoch\n",
    "    train_metric.reset()\n",
    "\n",
    "    # evaluating\n",
    "    test_str = eval(epoch, model, test_loader, test_metric)\n",
    "\n",
    "    print(f\"Epoch {epoch:04d} | TRAIN:{train_str} || TEST:{test_str} | Best: {config['training_params']['task'].title()} {test_metric.get_best_performance(config['training_params']['task']):.4f}\")\n",
    "\n",
    "    # task_dict = {\"train_loss\": train_metric.metric, \"test_loss\": test_metric.metric}\n",
    "    # np.save(\"logging/stl_{}_{}_{}_{}.npy\".format(config[\"training_params\"][\"network\"], config[\"training_params\"][\"dataset\"], config[\"training_params\"][\"task\"], config[\"training_params\"][\"seed\"]), task_dict)\n",
    "    torch_save(model, \"checkpoints/dinov2/linear_probing/depth_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-music-38</strong> at: <a href='https://wandb.ai/juagarci/simomm/runs/31xvtn2d' target=\"_blank\">https://wandb.ai/juagarci/simomm/runs/31xvtn2d</a><br/> View project at: <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">https://wandb.ai/juagarci/simomm</a><br/>Synced 5 W&B file(s), 0 media file(s), 19 artifact file(s) and 56 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_metas (list[dict]): List of image info dict where each dict\n",
    "#                 has: 'img_shape', 'scale_factor', 'flip', and may also contain\n",
    "#                 'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
