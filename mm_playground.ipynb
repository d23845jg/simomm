{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "from model_merging.aggregator import aggregate_task_vectors\n",
    "from model_merging.eval_utils import perform_eval_with_merged_vector\n",
    "from model_merging.task_vectors import MTLTaskVector\n",
    "from training.create_network import *\n",
    "from utils import initialize_wandb\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "template = env.get_template('config/mtl.yaml.j2')\n",
    "rendered_yaml = template.render()\n",
    "mm_config = yaml.safe_load(rendered_yaml)\n",
    "\n",
    "# Create logging folder to store training weights and losses\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "model_classes = {\n",
    "  \"split\": MTLDeepLabv3,\n",
    "  \"mtan\": MTANDeepLabv3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>logs/wandb/run-20241005_183520-uzm9ryva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/juagarci/simomm/runs/uzm9ryva' target=\"_blank\">feasible-deluge-24</a></strong> to <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">https://wandb.ai/juagarci/simomm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/juagarci/simomm/runs/uzm9ryva' target=\"_blank\">https://wandb.ai/juagarci/simomm/runs/uzm9ryva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'wandb' from '/opt/anaconda3/lib/python3.11/site-packages/wandb/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_wandb(\n",
    "  project=mm_config[\"wandb\"][\"project\"], \n",
    "  group=f\"{mm_config['training_params']['network']}\", \n",
    "  job_type=\"model_merging\", \n",
    "  mode=mm_config[\"wandb\"][\"mode\"], \n",
    "  config={\n",
    "    \"network\": mm_config['model_merging']['network'],\n",
    "    \"dataset\": mm_config['model_merging']['dataset'],\n",
    "    \"batch_size\": mm_config['training_params']['batch_size'],\n",
    "    \"ft_model_files\": mm_config['model_merging']['ft_model_files'],\n",
    "    \"method\": mm_config['model_merging']['method'],\n",
    "    \"seed\": mm_config['training_params']['seed'],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = model_classes[mm_config[\"model_merging\"][\"network\"]]({'seg': 13, 'depth': 1})\n",
    "# pt_model = torch_load(mm_config[\"model_merging\"][\"pt_model_file\"])\n",
    "task_vectors = [MTLTaskVector(pt_model, ft_file) for ft_file in mm_config[\"model_merging\"][\"ft_model_files\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of shared task vector:  tensor(627.8261)\n"
     ]
    }
   ],
   "source": [
    "mtl_task_vector = aggregate_task_vectors(task_vectors, mm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Nyuv2 | Training Task: Seg + Depth\n"
     ]
    }
   ],
   "source": [
    "train_tasks_str = ' + '.join(task.title() for task in mtl_task_vector.tasks.keys())\n",
    "print(f\"Dataset: {mm_config['model_merging']['dataset'].title()} | Training Task: {train_tasks_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "=========================================== alpha = 0.00 ===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.8901961].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.8901961].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.8901961].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.8901961].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluation time: 27.23s\n",
      "seg metric: nan | depth metric: 0.8344\n",
      "Delta MTL: nan\n",
      "\n",
      "\n",
      "\n",
      "=========================================== alpha = 0.25 ===========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m perform_eval_with_merged_vector(pt_model, mtl_task_vector, mm_config)\n",
      "File \u001b[0;32m~/Dropbox/Course/Year 2/Semester 1/Semester Project/repositories/simomm/model_merging/eval_utils.py:175\u001b[0m, in \u001b[0;36mperform_eval_with_merged_vector\u001b[0;34m(pt_checkpoint, task_vector, config, eval_masks)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_merging\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmag_masking\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# val_metrics = evaluate_task_vector(pt_checkpoint, task_vector, config, use_val_dataset=True, eval_masks=eval_masks)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# TODO: remove this\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m evaluate_task_vector(pt_checkpoint, task_vector, config, use_val_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_masks\u001b[38;5;241m=\u001b[39meval_masks)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_merging\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Dropbox/Course/Year 2/Semester 1/Semester Project/repositories/simomm/model_merging/eval_utils.py:113\u001b[0m, in \u001b[0;36mevaluate_task_vector\u001b[0;34m(pt_checkpoint, task_vector, config, use_val_dataset, eval_masks)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m43\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaling_coef\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m43\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m info[scaling_coef] \u001b[38;5;241m=\u001b[39m evaluate_task_vector_at_coef(\n\u001b[1;32m    114\u001b[0m     pt_checkpoint, task_vector, config, scaling_coef, use_val_dataset, eval_masks\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(info[scaling_coef][task][\u001b[38;5;241m1\u001b[39m],\u001b[38;5;250m \u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m task_vector\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mkeys()]))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta MTL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(info[scaling_coef][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/Course/Year 2/Semester 1/Semester Project/repositories/simomm/model_merging/eval_utils.py:65\u001b[0m, in \u001b[0;36mevaluate_task_vector_at_coef\u001b[0;34m(pt_checkpoint, task_vector, config, scaling_coef, use_val_dataset, eval_masks)\u001b[0m\n\u001b[1;32m     63\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m val_loader \u001b[38;5;28;01mif\u001b[39;00m use_val_dataset \u001b[38;5;28;01melse\u001b[39;00m test_loader\n\u001b[1;32m     64\u001b[0m test_metric \u001b[38;5;241m=\u001b[39m TaskMetric(model\u001b[38;5;241m.\u001b[39mtasks, model\u001b[38;5;241m.\u001b[39mtasks, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_merging\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m], include_mtl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m scaling_coef), model, data_loader, test_metric) \u001b[38;5;66;03m# 10 * scaling_coef is a hack to change it to an int\u001b[39;00m\n\u001b[1;32m     66\u001b[0m coef_metrics \u001b[38;5;241m=\u001b[39m test_metric\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m     68\u001b[0m coef_metrics \u001b[38;5;241m=\u001b[39m add_normalized_accuracy(coef_metrics, config)\n",
      "File \u001b[0;32m~/Dropbox/Course/Year 2/Semester 1/Semester Project/repositories/simomm/training/utils.py:347\u001b[0m, in \u001b[0;36meval\u001b[0;34m(epoch, model, data_loader, test_metric, device)\u001b[0m\n\u001b[1;32m    345\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(data_loader)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data_batch):\n\u001b[0;32m--> 347\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataset)\n\u001b[1;32m    348\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    349\u001b[0m     target \u001b[38;5;241m=\u001b[39m {task_id: target[task_id]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    945\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_eval_with_merged_vector(pt_model, mtl_task_vector, mm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762666e6a0b648a2a87e1438bc021284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.132 MB of 1.132 MB uploaded (0.055 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 4.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-deluge-24</strong> at: <a href='https://wandb.ai/juagarci/simomm/runs/uzm9ryva' target=\"_blank\">https://wandb.ai/juagarci/simomm/runs/uzm9ryva</a><br/> View project at: <a href='https://wandb.ai/juagarci/simomm' target=\"_blank\">https://wandb.ai/juagarci/simomm</a><br/>Synced 7 W&B file(s), 4 media file(s), 12 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seg + Depth\n",
    "\n",
    "=========================================== alpha = 0.00 ===========================================\n",
    "Total evaluation time: 101.41s\n",
    "seg metric: 0.2166 | depth metric: 0.8741\n",
    "Delta MTL: -0.39\n",
    "\n",
    "=========================================== alpha = 0.10 ===========================================\n",
    "Total evaluation time: 119.07s\n",
    "seg metric: 0.2545 | depth metric: 0.8934\n",
    "Delta MTL: -0.37\n",
    "\n",
    "=========================================== alpha = 0.50 ===========================================\n",
    "Total evaluation time: 111.14s\n",
    "seg metric: 0.2004 | depth metric: 1.052\n",
    "Delta MTL: -0.52\n",
    "\n",
    "...\n",
    "Just gets worst for larger alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seg Only\n",
    "\n",
    "=========================================== alpha = 0.00 ===========================================\n",
    "Total evaluation time: 80.16s\n",
    "seg metric: 0.2166\n",
    "Delta MTL: -0.17\n",
    "\n",
    "=========================================== alpha = 0.50 ===========================================\n",
    "Total evaluation time: 75.67s\n",
    "seg metric: 0.4218\n",
    "Delta MTL: -0.01\n",
    "\n",
    "=========================================== alpha = 1.00 ===========================================\n",
    "Total evaluation time: 80.37s\n",
    "seg metric: 0.4601\n",
    "Delta MTL: 0.02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth Only\n",
    "\n",
    "=========================================== alpha = 0.00 ===========================================\n",
    "Total evaluation time: 79.66s\n",
    "depth metric: 0.8741\n",
    "Delta MTL: -0.22\n",
    "\n",
    "=========================================== alpha = 0.50 ===========================================\n",
    "Total evaluation time: 80.56s\n",
    "depth metric: 1.0947\n",
    "Delta MTL: -0.37\n",
    "\n",
    "=========================================== alpha = 1.00 ===========================================\n",
    "Total evaluation time: 94.97s\n",
    "depth metric: 0.6197\n",
    "Delta MTL: -0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_save(model, 'logging/pt_model.pt')\n",
    "# torch_load('logging/model_test.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
